{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8334cd0",
   "metadata": {},
   "source": [
    "# Week 1: Perceptrons and MP Neuron (Deep Learning Foundations)\n",
    "\n",
    "These notes are prepared as a personal reference for thorough understanding before exams and coding practice. They include theory, diagrams, geometric intuition, and code demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5ce98",
   "metadata": {},
   "source": [
    "## 1. Biological Neuron Inspiration\n",
    "\n",
    "A biological neuron receives signals via dendrites, integrates them in the soma, and fires an output via the axon if the integrated signal exceeds a certain threshold.\n",
    "\n",
    "Key points:\n",
    "- Dendrites: Receive inputs.\n",
    "- Soma: Integrates signals.\n",
    "- Axon: Sends output signal.\n",
    "- Firing condition: Integrated signal exceeds a threshold.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/biological_neuron.png\" alt=\"Biological Neuron\" width=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f28506",
   "metadata": {},
   "source": [
    "## 2. McCulloch-Pitts (MP) Neuron\n",
    "\n",
    "- Inputs: $x_i \\in \\{0, 1\\}$.\n",
    "- Output: $y \\in \\{0, 1\\}$.\n",
    "- Aggregation function: $g(x) = \\sum_{i=1}^n x_i$.\n",
    "- Fires if $g(x) \\geq \\theta$.\n",
    "\n",
    "Inhibitory inputs:\n",
    "- If any inhibitory input is 1, neuron output is forced to 0.\n",
    "\n",
    "Examples of logic gates:\n",
    "- AND: Fires only if all inputs are 1.\n",
    "- OR: Fires if at least one input is 1.\n",
    "- NOR: Fires only if all inputs are 0.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/mp_neuron.png\" alt=\"MP Neuron\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/logic_gates.png\" alt=\"Logic Gates\" width=\"450\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bc93f",
   "metadata": {},
   "source": [
    "## 3. Boolean Functions and Linearity\n",
    "\n",
    "- MP neurons can implement linearly separable Boolean functions.\n",
    "- Functions like XOR cannot be implemented using a single MP neuron because they are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de773fb",
   "metadata": {},
   "source": [
    "## 4. Geometric Interpretation\n",
    "\n",
    "- Decision boundary: $w \\cdot x + b = 0$.\n",
    "- Weight vector $w$ is orthogonal (perpendicular) to this hyperplane.\n",
    "- Points satisfying $w \\cdot x + b = 0$ lie on the boundary.\n",
    "- Moving in direction of $w$ changes the score (activation) fastest.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/geometric_boundary.png\" alt=\"Geometric Boundary\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/orthogonal_view.png\" alt=\"Orthogonal View\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebaa9c",
   "metadata": {},
   "source": [
    "## 5. Perceptron Model\n",
    "\n",
    "- Inputs: $x \\in \\mathbb{R}^n$ or $\\{0, 1\\}^n$.\n",
    "- Weights: Real-valued, learned during training.\n",
    "- Bias term shifts the decision boundary.\n",
    "\n",
    "Decision function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & w \\cdot x + b \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Perceptron Learning Algorithm\n",
    "\n",
    "- Misclassified condition: $y (w \\cdot x + b) \\leq 0$.\n",
    "- Update rule:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w + \\eta y x, \\quad b \\leftarrow b + \\eta y\n",
    "$$\n",
    "\n",
    "- Learning rate $\\eta$ controls update step size.\n",
    "- Algorithm converges if data is linearly separable.\n",
    "\n",
    "Why it works:\n",
    "- Update increases $y (w \\cdot x + b)$, pushing points toward the correct side of the boundary.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/perceptron_update.png\" alt=\"Perceptron Update\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63b891",
   "metadata": {},
   "source": [
    "## 6. MP Neuron vs Perceptron Comparison\n",
    "\n",
    "MP neuron:\n",
    "- Binary weights and inputs.\n",
    "- No learning, static logic.\n",
    "- Can only handle linearly separable functions.\n",
    "\n",
    "Perceptron:\n",
    "- Real-valued weights, learns from data.\n",
    "- Can handle both real and binary inputs.\n",
    "- Includes a bias term for flexible boundary adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79740b2",
   "metadata": {},
   "source": [
    "## 7. Python Example: Perceptron Learning on AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([-1, -1, -1, 1])  # AND gate labeling\n",
    "\n",
    "w = np.zeros(2)\n",
    "b = 0\n",
    "eta = 1\n",
    "\n",
    "for epoch in range(10):\n",
    "    for xi, target in zip(X, y):\n",
    "        if target * (np.dot(xi, w) + b) <= 0:\n",
    "            w += eta * target * xi\n",
    "            b += eta * target\n",
    "    print(f\"Epoch {epoch+1}: w = {w}, b = {b}\")\n",
    "\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7cee4",
   "metadata": {},
   "source": [
    "## 8. Optional: Geometric Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df34dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "x_vals = np.linspace(-0.5, 1.5, 100)\n",
    "y_vals = -x_vals + 0.5\n",
    "\n",
    "plt.scatter(X[y==0][:,0], X[y==0][:,1], c='red', label='Class 0')\n",
    "plt.scatter(X[y==1][:,0], X[y==1][:,1], c='blue', label='Class 1')\n",
    "plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n",
    "\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('2D Decision Boundary Example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1860a6",
   "metadata": {},
   "source": [
    "## 9. Final Revision Points\n",
    "\n",
    "- MP neuron uses fixed threshold sum; no learning mechanism.\n",
    "- Perceptron learns linear decision boundaries through updates.\n",
    "- Weight vector is orthogonal to decision hyperplane.\n",
    "- Bias allows flexible shifting of the boundary.\n",
    "- Update rule derived to correct misclassifications and improve margin."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
